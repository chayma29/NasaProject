{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "699b131b",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a32f34fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (accuracy_score, precision_score, recall_score, \n\u001b[0;32m      7\u001b[0m                              f1_score, confusion_matrix, classification_report)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ====================================================================\n",
    "# Ã‰TAPE 7 - XGBOOST AVEC GRIDSEARCH ET Ã‰VALUATION COMPLÃˆTE\n",
    "# ====================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Ã‰TAPE 7 - XGBOOST AVEC GRIDSEARCH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. CHARGEMENT DES DONNÃ‰ES\n",
    "print(\"\\n1. CHARGEMENT DES DONNÃ‰ES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "X_train = pd.read_csv('C:/Users/chaym/Desktop/NasaProject/data/processed/step6_X_train.csv')\n",
    "X_val = pd.read_csv('C:/Users/chaym/Desktop/NasaProject/data/processed/step6_X_val.csv')\n",
    "y_train = pd.read_csv('C:/Users/chaym/Desktop/NasaProject/data/processed/step6_y_train.csv')\n",
    "y_val = pd.read_csv('C:/Users/chaym/Desktop/NasaProject/data/processed/step6_y_val.csv')\n",
    "\n",
    "# Convertir en array 1D pour les targets\n",
    "y_train = y_train.values.ravel()\n",
    "y_val = y_val.values.ravel()\n",
    "\n",
    "print(f\"âœ“ X_train: {X_train.shape}\")\n",
    "print(f\"âœ“ X_val: {X_val.shape}\")\n",
    "print(f\"âœ“ y_train: {y_train.shape}\")\n",
    "print(f\"âœ“ y_val: {y_val.shape}\")\n",
    "\n",
    "# 2. DÃ‰FINITION DE LA GRILLE D'HYPERPARAMÃˆTRES\n",
    "print(\"\\n2. DÃ‰FINITION DE LA GRILLE D'HYPERPARAMÃˆTRES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "print(\"âœ“ ParamÃ¨tres Ã  tester:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  â€¢ {param}: {values}\")\n",
    "\n",
    "total_combinations = np.prod([len(v) for v in param_grid.values()])\n",
    "print(f\"\\nâœ“ Nombre total de combinaisons: {total_combinations}\")\n",
    "\n",
    "# 3. GRIDSEARCH AVEC VALIDATION CROISÃ‰E\n",
    "print(\"\\n3. GRIDSEARCH AVEC VALIDATION CROISÃ‰E\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# ModÃ¨le de base\n",
    "base_model = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=3,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    enable_categorical=False\n",
    ")\n",
    "\n",
    "# GridSearch\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"ðŸ” DÃ©marrage du GridSearch...\")\n",
    "print(\"   (Cela peut prendre plusieurs minutes)\")\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nâœ“ GridSearch terminÃ© en {elapsed_time/60:.2f} minutes\")\n",
    "\n",
    "# 4. MEILLEURS PARAMÃˆTRES\n",
    "print(\"\\n4. MEILLEURS PARAMÃˆTRES TROUVÃ‰S\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"âœ“ Meilleurs hyperparamÃ¨tres:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  â€¢ {param}: {value}\")\n",
    "\n",
    "print(f\"\\nâœ“ Meilleur score (F1 weighted) CV: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Sauvegarder les rÃ©sultats du GridSearch\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results.to_csv('C:/Users/chaym/Desktop/NasaProject/data/processed/step7_gridsearch_results.csv', index=False)\n",
    "print(\"âœ“ RÃ©sultats GridSearch sauvegardÃ©s: step7_gridsearch_results.csv\")\n",
    "\n",
    "# 5. ENTRAÃŽNEMENT DU MODÃˆLE FINAL AVEC COURBES DE LOSS\n",
    "print(\"\\n5. ENTRAÃŽNEMENT DU MODÃˆLE FINAL AVEC EVAL SET\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "best_model = XGBClassifier(\n",
    "    **grid_search.best_params_,\n",
    "    objective='multi:softmax',\n",
    "    num_class=3,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    enable_categorical=False,\n",
    "    early_stopping_rounds=20\n",
    ")\n",
    "\n",
    "# EntraÃ®nement avec eval_set pour tracer les courbes\n",
    "eval_set = [(X_train, y_train), (X_val, y_val)]\n",
    "eval_names = ['train', 'val']\n",
    "\n",
    "best_model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    eval_set=eval_set,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"âœ“ ModÃ¨le entraÃ®nÃ© avec {best_model.n_estimators} estimateurs\")\n",
    "print(f\"âœ“ Meilleure itÃ©ration: {best_model.best_iteration}\")\n",
    "\n",
    "# 6. PRÃ‰DICTIONS\n",
    "print(\"\\n6. PRÃ‰DICTIONS SUR TRAIN ET VAL\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "print(\"âœ“ PrÃ©dictions effectuÃ©es\")\n",
    "\n",
    "# 7. MÃ‰TRIQUES D'Ã‰VALUATION\n",
    "print(\"\\n7. MÃ‰TRIQUES D'Ã‰VALUATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, dataset_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'Dataset': dataset_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    }\n",
    "\n",
    "metrics_train = calculate_metrics(y_train, y_train_pred, \"TRAIN\")\n",
    "metrics_val = calculate_metrics(y_val, y_val_pred, \"VAL\")\n",
    "\n",
    "# Sauvegarder les mÃ©triques\n",
    "metrics_df = pd.DataFrame([metrics_train, metrics_val])\n",
    "metrics_df.to_csv('C:/Users/chaym/Desktop/NasaProject/data/processed/step7_metrics.csv', index=False)\n",
    "print(\"\\nâœ“ MÃ©triques sauvegardÃ©es: step7_metrics.csv\")\n",
    "\n",
    "# 8. CLASSIFICATION REPORT DÃ‰TAILLÃ‰\n",
    "print(\"\\n8. CLASSIFICATION REPORT DÃ‰TAILLÃ‰\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "label_names = ['FALSE POSITIVE', 'CANDIDATE', 'CONFIRMED']\n",
    "\n",
    "print(\"\\nTRAIN SET:\")\n",
    "print(classification_report(y_train, y_train_pred, target_names=label_names, digits=4))\n",
    "\n",
    "print(\"\\nVAL SET:\")\n",
    "print(classification_report(y_val, y_val_pred, target_names=label_names, digits=4))\n",
    "\n",
    "# 9. VISUALISATION DES COURBES DE LOSS\n",
    "print(\"\\n9. VISUALISATION DES COURBES DE LOSS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# RÃ©cupÃ©rer les rÃ©sultats d'Ã©valuation\n",
    "results = best_model.evals_result()\n",
    "\n",
    "# Courbe de loss\n",
    "ax1 = axes[0]\n",
    "epochs = range(len(results['validation_0']['mlogloss']))\n",
    "ax1.plot(epochs, results['validation_0']['mlogloss'], label='Train Loss', linewidth=2, color='#3498db')\n",
    "ax1.plot(epochs, results['validation_1']['mlogloss'], label='Val Loss', linewidth=2, color='#e74c3c')\n",
    "ax1.axvline(x=best_model.best_iteration, color='green', linestyle='--', \n",
    "            label=f'Best Iteration ({best_model.best_iteration})', linewidth=2)\n",
    "ax1.set_xlabel('Iterations', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Multiclass Log Loss', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Courbes de Loss - Train vs Val', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Courbe de loss (zoom sur les derniÃ¨res itÃ©rations)\n",
    "ax2 = axes[1]\n",
    "start_idx = max(0, len(epochs) - 100)\n",
    "ax2.plot(epochs[start_idx:], results['validation_0']['mlogloss'][start_idx:], \n",
    "         label='Train Loss', linewidth=2, color='#3498db')\n",
    "ax2.plot(epochs[start_idx:], results['validation_1']['mlogloss'][start_idx:], \n",
    "         label='Val Loss', linewidth=2, color='#e74c3c')\n",
    "if best_model.best_iteration >= start_idx:\n",
    "    ax2.axvline(x=best_model.best_iteration, color='green', linestyle='--', \n",
    "                label=f'Best Iteration', linewidth=2)\n",
    "ax2.set_xlabel('Iterations', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Multiclass Log Loss', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Courbes de Loss - Zoom (derniÃ¨res 100 itÃ©rations)', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('C:/Users/chaym/Desktop/NasaProject/data/processed/step7_loss_curves.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ“ Courbes de loss sauvegardÃ©es: step7_loss_curves.png\")\n",
    "plt.close()\n",
    "\n",
    "# 10. MATRICES DE CONFUSION\n",
    "print(\"\\n10. MATRICES DE CONFUSION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Matrice de confusion TRAIN\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_names, yticklabels=label_names,\n",
    "            ax=axes[0], cbar_kws={'label': 'Nombre de prÃ©dictions'})\n",
    "axes[0].set_title('Matrice de Confusion - TRAIN', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Vraie Classe', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Classe PrÃ©dite', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Matrice de confusion VAL\n",
    "cm_val = confusion_matrix(y_val, y_val_pred)\n",
    "sns.heatmap(cm_val, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=label_names, yticklabels=label_names,\n",
    "            ax=axes[1], cbar_kws={'label': 'Nombre de prÃ©dictions'})\n",
    "axes[1].set_title('Matrice de Confusion - VAL', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Vraie Classe', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Classe PrÃ©dite', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('C:/Users/chaym/Desktop/NasaProject/data/processed/step7_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ“ Matrices de confusion sauvegardÃ©es: step7_confusion_matrices.png\")\n",
    "plt.close()\n",
    "\n",
    "# 11. IMPORTANCE DES FEATURES\n",
    "print(\"\\n11. IMPORTANCE DES FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': best_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 features les plus importantes:\")\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Features', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 15 Features les Plus Importantes (XGBoost)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('C:/Users/chaym/Desktop/NasaProject/data/processed/step7_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ“ Feature importance sauvegardÃ©e: step7_feature_importance.png\")\n",
    "plt.close()\n",
    "\n",
    "# Sauvegarder l'importance des features\n",
    "feature_importance.to_csv('C:/Users/chaym/Desktop/NasaProject/data/processed/step7_feature_importance.csv', index=False)\n",
    "print(\"âœ“ Feature importance CSV: step7_feature_importance.csv\")\n",
    "\n",
    "# 12. COMPARAISON TRAIN VS VAL\n",
    "print(\"\\n12. COMPARAISON TRAIN VS VAL\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "train_scores = [metrics_train[m] for m in metrics_names]\n",
    "val_scores = [metrics_val[m] for m in metrics_names]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, train_scores, width, label='Train', color='#3498db', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, val_scores, width, label='Val', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Comparaison des MÃ©triques - Train vs Val', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_names)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 1.1])\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('C:/Users/chaym/Desktop/NasaProject/data/processed/step7_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ“ Comparaison des mÃ©triques sauvegardÃ©e: step7_metrics_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "# 13. SAUVEGARDE DU MODÃˆLE\n",
    "print(\"\\n13. SAUVEGARDE DU MODÃˆLE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "import joblib\n",
    "joblib.dump(best_model, 'C:/Users/chaym/Desktop/NasaProject/data/processed/step7_xgboost_model.pkl')\n",
    "print(\"âœ“ ModÃ¨le sauvegardÃ©: step7_xgboost_model.pkl\")\n",
    "\n",
    "# 14. RÃ‰SUMÃ‰ FINAL\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RÃ‰SUMÃ‰ FINAL - XGBOOST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ MEILLEURS HYPERPARAMÃˆTRES:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  â€¢ {param}: {value}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š PERFORMANCES:\")\n",
    "print(f\"  TRAIN:\")\n",
    "print(f\"    â€¢ Accuracy:  {metrics_train['Accuracy']:.4f}\")\n",
    "print(f\"    â€¢ F1-Score:  {metrics_train['F1-Score']:.4f}\")\n",
    "print(f\"  VAL:\")\n",
    "print(f\"    â€¢ Accuracy:  {metrics_val['Accuracy']:.4f}\")\n",
    "print(f\"    â€¢ F1-Score:  {metrics_val['F1-Score']:.4f}\")\n",
    "\n",
    "overfitting = metrics_train['F1-Score'] - metrics_val['F1-Score']\n",
    "print(f\"\\n  ðŸ“‰ Ã‰cart Train-Val (F1): {overfitting:.4f}\")\n",
    "if overfitting < 0.05:\n",
    "    print(f\"     âœ… Pas d'overfitting significatif!\")\n",
    "elif overfitting < 0.10:\n",
    "    print(f\"     âš ï¸  LÃ©ger overfitting\")\n",
    "else:\n",
    "    print(f\"     âŒ Overfitting dÃ©tectÃ©!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Ã‰TAPE 7 TERMINÃ‰E âœ“\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“ Fichiers gÃ©nÃ©rÃ©s:\")\n",
    "print(\"  1. step7_gridsearch_results.csv - RÃ©sultats du GridSearch\")\n",
    "print(\"  2. step7_metrics.csv - MÃ©triques Train/Val\")\n",
    "print(\"  3. step7_loss_curves.png - Courbes de loss\")\n",
    "print(\"  4. step7_confusion_matrices.png - Matrices de confusion\")\n",
    "print(\"  5. step7_feature_importance.png - Importance des features\")\n",
    "print(\"  6. step7_feature_importance.csv - Importance (CSV)\")\n",
    "print(\"  7. step7_metrics_comparison.png - Comparaison mÃ©triques\")\n",
    "print(\"  8. step7_xgboost_model.pkl - ModÃ¨le entraÃ®nÃ©\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Prochaines Ã©tapes:\")\n",
    "print(\"  1. Analyser les rÃ©sultats et l'overfitting\")\n",
    "print(\"  2. Tester d'autres modÃ¨les (Random Forest, etc.)\")\n",
    "print(\"  3. Ã‰valuation finale sur le test set\")\n",
    "print(\"  4. InterprÃ©tation des features importantes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
